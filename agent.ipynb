{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chungsp2003/nv-blockchain-bluemix/blob/master/agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXY7cnBM2Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gX9uvnOZNx",
        "colab_type": "code",
        "outputId": "9dc329a0-a1f7-4b48-b5ee-4633d03e261a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "pip install wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.8.32)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.2)\n",
            "Requirement already satisfied: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.14.3)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.4.5.1)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJT61wdgX0aO",
        "colab_type": "code",
        "outputId": "f34677de-d80f-43ed-c968-cf3aad4c3234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "pip install Q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Q\n",
            "  Downloading https://files.pythonhosted.org/packages/53/bc/51619d89e0bd855567e7652fa16d06f1ed36a85f108a7fe71f6629bf719d/q-2.6-py2.py3-none-any.whl\n",
            "Installing collected packages: Q\n",
            "Successfully installed Q-2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6mlKGKMXVlX",
        "colab_type": "code",
        "outputId": "06bbea03-9b81-4b88-cd3a-9e0fe41f0503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "\"\"\"\n",
        "Deep Q Learning (2020.4.12)\n",
        "\"\"\"\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from model import Q\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self,\n",
        "        env,\n",
        "        hidden_dim,\n",
        "        lr,\n",
        "        log_every=200,\n",
        "        buffer_size = 100000,\n",
        "        epsilon = 0.3,\n",
        "        batch_size=512,\n",
        "        explore_to=200,\n",
        "        train_from=100,\n",
        "        update_target_every=50,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Lovely Class\n",
        "        \"\"\"\n",
        "        self.q = Q(hidden_dim)\n",
        "        self.q_target = Q(hidden_dim)\n",
        "        self.q_target.load_state_dict(self.q.state_dict())\n",
        "        self.optimizer = optim.Adam(self.q.parameters(), lr=lr)\n",
        "        self.discount_factor = 0.99\n",
        "        self.env = env\n",
        "        self.log_every = log_every\n",
        "        self.buffer_size = buffer_size\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.explore_to = explore_to\n",
        "        self.train_from = train_from\n",
        "        self.update_target_every = update_target_every\n",
        "\n",
        "        self.counter = 0\n",
        "        self.counter_data = 0 \n",
        "\n",
        "        self.initialize_data()\n",
        "\n",
        "    def put_data(self,\n",
        "        state,\n",
        "        action,\n",
        "        reward,\n",
        "        next_state,\n",
        "        done):\n",
        "\n",
        "        self.data['states'][self.counter_data] = state\n",
        "        self.data['actions'][self.counter_data] = action\n",
        "        self.data['rewards'][self.counter_data] = reward\n",
        "        self.data['next_states'][self.counter_data] = next_state\n",
        "        self.data['dones'][self.counter_data] = done\n",
        "\n",
        "        self.counter += 1\n",
        "        self.counter = min(self.counter, self.buffer_size)\n",
        "\n",
        "        self.counter_data +=1\n",
        "        self.counter_data = self.counter_data % self.buffer_size\n",
        "\n",
        "    def initialize_data(self):\n",
        "        self.data = dict()\n",
        "        self.data['states'] = np.zeros((self.buffer_size, 4))\n",
        "        self.data['actions'] = np.zeros((self.buffer_size, 1))\n",
        "        self.data['rewards'] = np.zeros((self.buffer_size, 1))\n",
        "        self.data['next_states'] = np.zeros((self.buffer_size, 4))\n",
        "        self.data['dones'] = np.zeros((self.buffer_size, 1))\n",
        "\n",
        "    def interact(self, start=False):\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if start or random.random() < self.epsilon:\n",
        "                action = random.choice(range(2))\n",
        "            else:\n",
        "                action_values= self.q(torch.tensor(state,\n",
        "                                                 dtype=torch.float32).unsqueeze(0))\n",
        "                action = torch.argmax(action_values).item()\n",
        "\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "            self.put_data(state,\n",
        "                          action,\n",
        "                          reward,\n",
        "                          next_state,\n",
        "                          done * 1.)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    def get_data_from_buffer(self, batch_size=32):\n",
        "        indices = np.random.choice(range(self.counter), batch_size)\n",
        "        \n",
        "        states = self.data['states'][indices]\n",
        "        actions = self.data['actions'][indices]\n",
        "        rewards = self.data['rewards'][indices]\n",
        "        next_states = self.data['next_states'][indices]\n",
        "        dones = self.data['dones'][indices]\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def step(self):\n",
        "        states, actions, rewards, next_states, dones = self.get_data_from_buffer(self.batch_size)\n",
        "\n",
        "        values = self.q(states).gather(1, actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            targets = rewards +\\\n",
        "                      self.discount_factor*self.q_target(next_states).max(dim=1, keepdim=True)[0]*\\\n",
        "                      (1-dones)\n",
        "\n",
        "        loss = (values-targets).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self, iterations):\n",
        "        for i in range(1, iterations+1):\n",
        "            if i < self.explore_to:\n",
        "                self.interact(True)\n",
        "            else:\n",
        "                self.interact(False)\n",
        "            \n",
        "            if i < self.train_from:\n",
        "                pass\n",
        "            else:\n",
        "                self.step()\n",
        "\n",
        "            if i % self.log_every == 0:\n",
        "                wandb.log(self.evaluation())\n",
        "\n",
        "            if i % self.update_target_every == 0:\n",
        "                self.update_target()\n",
        "\n",
        "    def evaluation(self):\n",
        "        total_counter = []\n",
        "        with torch.no_grad():\n",
        "            counter = 0\n",
        "            for i in range(30):\n",
        "                state = self.env.reset()\n",
        "                done = False\n",
        "                \n",
        "                while not done:\n",
        "                    action_values= self.q(torch.tensor(state,\n",
        "                                                         dtype=torch.float32).unsqueeze(0))\n",
        "                    action = torch.argmax(action_values).item()\n",
        "                    next_state, reward, done, _ = self.env.step(action)\n",
        "                    counter += reward\n",
        "                    state = next_state\n",
        "\n",
        "                total_counter.append(counter)\n",
        "                counter = 0\n",
        "\n",
        "        return {\n",
        "                \"Reward (Meidian)\": np.median(total_counter),\n",
        "                \"Reward (Mean)\": np.mean(total_counter),\n",
        "                \"Reward (Max)\": np.max(total_counter),\n",
        "                \"Reward (Standard Deviation\":  np.std(total_counter),\n",
        "        }\n",
        "\n",
        "    def update_target(self):\n",
        "        self.q_target.load_state_dict(self.q.state_dict())\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.q.state_dict(), 'q.pth')\n",
        "        torch.save(self.q_target.state_dict(), 'q_target.pth')\n",
        "        torch.save(self.optimizer.state_dict(), 'optimizer.pth')\n",
        "\n",
        "    def load(self):\n",
        "        self.q.load_state_dict(torch.load('q.pth'))\n",
        "        self.q_target.load_state_dict(torch.load('q_target.pth'))\n",
        "        self.optimizer.load_state_dict(torch.load('optimizer.pth'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-072e34fa9444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}